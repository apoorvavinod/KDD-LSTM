{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import decomposition\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import scikitplot as skplt\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy import interp\n",
    "from itertools import cycle\n",
    "import time\n",
    "from keras import metrics\n",
    "from keras import optimizers\n",
    "from keras import initializers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kdd_dataset = pd.read_csv('KDDpreProcessed.csv')\n",
    "np.random.seed(42)\n",
    "kdd_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = list(kdd_dataset)\n",
    "features.remove('label_DoS')\n",
    "features.remove('label_Normal')\n",
    "features.remove('label_Probe')\n",
    "features.remove('label_R2L')\n",
    "features.remove('label_U2R')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "DoS = kdd_dataset.loc[kdd_dataset['label_DoS'] == 1]\n",
    "Normal = kdd_dataset.loc[kdd_dataset['label_Normal'] == 1]\n",
    "Probe = kdd_dataset.loc[kdd_dataset['label_Probe'] == 1]\n",
    "R2L = kdd_dataset.loc[kdd_dataset['label_R2L'] == 1]\n",
    "U2R = kdd_dataset.loc[kdd_dataset['label_U2R'] == 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "DoS = DoS.sample(n=97084,random_state = 42)\n",
    "Normal = Normal.sample(n=24320, random_state = 42)\n",
    "Probe = Probe.sample(n=1027, random_state=42)\n",
    "R2L = R2L.sample(n=1126, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Optional - Oversample the imbalanced class\n",
    "\n",
    "#U2R = U2R.append([U2R]*500,ignore_index=True)\n",
    "#R2L = R2L.append([R2L]*100,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reduced_dataset = pd.concat([DoS,Normal,Probe,R2L,U2R])\n",
    "kdd_dataset = reduced_dataset.sample(n=len(reduced_dataset), random_state = 42)\n",
    "#kdd_dataset = kdd_dataset.sample(n=len(kdd_dataset), random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of datapoints = 123579 and number of features = 53\n"
     ]
    }
   ],
   "source": [
    "x = kdd_dataset[features].values\n",
    "y = kdd_dataset.iloc[:,53:].values\n",
    "print(\"number of datapoints = {} and number of features = {}\".format(len(x),len(x[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({(1, 0, 0, 0, 0): 97084, (0, 1, 0, 0, 0): 24320, (0, 0, 0, 1, 0): 1126, (0, 0, 1, 0, 0): 1027, (0, 0, 0, 0, 1): 22})\n"
     ]
    }
   ],
   "source": [
    "# Counting occurrences\n",
    "from collections import Counter\n",
    "print(Counter([tuple(x) for x in y]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before transformation - (123579, 53)\n",
      "Shape after transformation - (123579, 10)\n"
     ]
    }
   ],
   "source": [
    "#PCA\n",
    "\n",
    "print(\"Shape before transformation - {}\".format(np.asarray(x).shape))\n",
    "pca = decomposition.PCA(n_components=10)\n",
    "pca.fit(x)\n",
    "x_pca = pca.transform(x)\n",
    "print(\"Shape after transformation - {}\".format(x_pca.shape))\n",
    "x_pca = x_pca.tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_tr, X_val, Y_tr, Y_val = train_test_split(x_pca, y,\n",
    "                                                    stratify=y, \n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_tr, Y_tr,\n",
    "                                                    stratify=Y_tr, \n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X_train, X_test, Y_train, Y_test = train_test_split(x_pca, y, test_size=0.20, random_state=42)\n",
    "#print(\"Length of training data = {} and Length of test data = {}\".format(len(X_train),len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X_train=np.asarray(X_train)\n",
    "#X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Y_train=np.asarray(Y_train)\n",
    "#Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#x=np.asarray(x)\n",
    "#xx=np.asarray(xx)\n",
    "#print(xx.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Y_train=np.reshape(Y_train, (Y_train.shape[0], 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#x.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nskf = StratifiedKFold(n_splits=15)\\nskf.get_n_splits(x_kbest, y)\\n#print(skf)  \\n\\nfor train_index, test_index in skf.split(x_kbest, y):\\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\\n    X_train, X_test = x_kbest[train_index], x_kbest[test_index]\\n    y_train, y_test = y[train_index], y[test_index]\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "skf = StratifiedKFold(n_splits=15)\n",
    "skf.get_n_splits(x_kbest, y)\n",
    "#print(skf)  \n",
    "\n",
    "for train_index, test_index in skf.split(x_kbest, y):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = x_kbest[train_index], x_kbest[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X_train.shape[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def processTrainTestArrays(x,y):\n",
    "    x=np.asarray(x)\n",
    "    y=np.asarray(y)\n",
    "    #x = np.reshape(x, (x.shape[0], 1, x.shape[1]))\n",
    "    #y=np.reshape(y, (y.shape[0], y.shape[1]))\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def onehotencode(y):\n",
    "    from keras.utils.np_utils import to_categorical\n",
    "    y_binary = to_categorical(y)\n",
    "    return y_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define model\n",
    "#length = len(kdd_dataset)\n",
    "def create_model(X_train,Y_train):\n",
    "    length = 1\n",
    "    #n_features = len(kdd_dataset.iloc[0])-1\n",
    "    #n_features = len(kdd_dataset.iloc[0])\n",
    "    randomInit = initializers.RandomUniform(seed=42)\n",
    "    number_of_units = 50\n",
    "    model = Sequential()\n",
    "    model.add(Dense(number_of_units, kernel_initializer = randomInit, input_dim=X_train.shape[1], activation = 'relu'))\n",
    "    #model.add(Dense(n_features, activation='softmax'))\n",
    "    #model.add(Dense(x.shape[2], activation='relu'))\n",
    "\n",
    "    #model.add(Dense(number_of_units,kernel_initializer = randomInit, activation = 'relu'))\n",
    "    #model.add(Dense(number_of_units,kernel_initializer = randomInit, activation = 'relu'))\n",
    "\n",
    "    #model.add(LSTM(50, input_shape=(X_train.shape[1], X_train.shape[2]), activation = 'relu'))\n",
    "    model.add(Dense(Y_train.shape[1], activation='softmax'))\n",
    "    #model.add(Dense(X_train.shape[1], activation='softmax'))\n",
    "\n",
    "\n",
    "    #model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    #model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    #model.compile(loss='categorical_crossentropy', optimizer='nadam', metrics=['accuracy',metrics.categorical_accuracy,'categorical_accuracy'])\n",
    "    nadam = optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=nadam, metrics=['acc'])\n",
    "\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_metrics(predicted, Y_test):\n",
    "    \n",
    "    Y_classes = [np.argmax(item) for item in Y_test]\n",
    "    #print(Y_classes)\n",
    "    predicted_classes = [np.argmax(item) for item in predicted]\n",
    "    \n",
    "    predictions = np.array(predicted_classes)\n",
    "    Y_actual = np.array(Y_classes)\n",
    "    \n",
    "    \n",
    "\n",
    "    y_actu = pd.Series(np.reshape(Y_actual,len(Y_actual)), name='Actual')\n",
    "    y_pred = pd.Series(np.reshape(predictions, len(predictions)), name='Predicted')\n",
    "    \n",
    "    \n",
    "    y_act = pd.Categorical(y_actu, categories=[0,1,2,3,4])\n",
    "    y_pre = pd.Categorical(y_pred, categories=[0,1,2,3,4])\n",
    " \n",
    "    #F1 score\n",
    "    from sklearn.metrics import f1_score\n",
    "    \n",
    "    \n",
    "    print(\"Average F1 score is {}\".format(f1_score(Y_classes, predicted_classes, average='weighted', labels = [0,1,2,3,4])))\n",
    "   \n",
    "    df_confusion = pd.crosstab(y_act, y_pre, rownames =['Actual'],colnames=['Predicted'])\n",
    "    \n",
    "    \n",
    "    print(\"Confusion matrix: \\n\", df_confusion)\n",
    "    #pprint.pprint(df_confusion)\n",
    "    predicted= np.array(predicted).tolist()\n",
    "    Y_test = np.array(Y_test).tolist()\n",
    "\n",
    "    \n",
    "    n_classes = 5\n",
    "    \n",
    "    print(\"n_classes is: \", n_classes)\n",
    "    Y_test= np.asarray(Y_test)\n",
    "    predicted = np.asarray(predicted)\n",
    "   \n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "        #print(\"i is *********************\", i)\n",
    "        #print(Y_test[:, i], predicted[:, i])\n",
    "        fpr[i], tpr[i], _ = roc_curve(Y_test[:, i], predicted[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(Y_test.ravel(), predicted.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "    \n",
    "    lw = 2\n",
    "\n",
    "    \n",
    "     # Compute macro-average ROC curve and ROC area\n",
    "\n",
    "    # First aggregate all false positive rates\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "    # Then interpolate all ROC curves at this points\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(n_classes):\n",
    "        mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "    # Finally average it and compute AUC\n",
    "    mean_tpr /= n_classes\n",
    "\n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "    # Plot all ROC curves\n",
    "    plt.figure()\n",
    "    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "             label='micro-average ROC curve (area = {0:0.2f})'\n",
    "                   ''.format(roc_auc[\"micro\"]),\n",
    "             color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "    plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "             label='macro-average ROC curve (area = {0:0.2f})'\n",
    "                   ''.format(roc_auc[\"macro\"]),\n",
    "             color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "    colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "                 label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "                 ''.format(i, roc_auc[i]))\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#checkpoint save weights\n",
    "#Y_train\n",
    "hdf5FileName = \"bestWeightsMLP1L50.hdf5\"\n",
    "checkpoint = ModelCheckpoint(hdf5FileName,monitor='val_acc',verbose=1,save_best_only=True,mode='max',save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 50)                550       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 255       \n",
      "=================================================================\n",
      "Total params: 805\n",
      "Trainable params: 805\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 74147 samples, validate on 24716 samples\n",
      "Epoch 1/300\n",
      "69632/74147 [===========================>..] - ETA: 0s - loss: 3.9907 - acc: 0.7511- ETA: 0s - loss: 8.4914 - acc: 0Epoch 00001: val_acc improved from -inf to 0.98155, saving model to bestWeightsMLP1L50.hdf5\n",
      "74147/74147 [==============================] - 1s 14us/step - loss: 3.7638 - acc: 0.7653 - val_loss: 0.2944 - val_acc: 0.9816\n",
      "Epoch 2/300\n",
      "69760/74147 [===========================>..] - ETA: 0s - loss: 0.2980 - acc: 0.9812Epoch 00002: val_acc improved from 0.98155 to 0.98159, saving model to bestWeightsMLP1L50.hdf5\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.2958 - acc: 0.9814 - val_loss: 0.2942 - val_acc: 0.9816\n",
      "Epoch 3/300\n",
      "71040/74147 [===========================>..] - ETA: 0s - loss: 0.2968 - acc: 0.9814Epoch 00003: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.2955 - acc: 0.9815 - val_loss: 0.2942 - val_acc: 0.9816\n",
      "Epoch 4/300\n",
      "70912/74147 [===========================>..] - ETA: 0s - loss: 0.2978 - acc: 0.9812Epoch 00004: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.2961 - acc: 0.9813 - val_loss: 0.2944 - val_acc: 0.9816\n",
      "Epoch 5/300\n",
      "70272/74147 [===========================>..] - ETA: 0s - loss: 0.2981 - acc: 0.9813Epoch 00005: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.2956 - acc: 0.9815 - val_loss: 0.2952 - val_acc: 0.9816\n",
      "Epoch 6/300\n",
      "70912/74147 [===========================>..] - ETA: 0s - loss: 0.2993 - acc: 0.9812Epoch 00006: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.2976 - acc: 0.9813 - val_loss: 0.2944 - val_acc: 0.9814\n",
      "Epoch 7/300\n",
      "71168/74147 [===========================>..] - ETA: 0s - loss: 0.2970 - acc: 0.9813Epoch 00007: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.2955 - acc: 0.9814 - val_loss: 0.2953 - val_acc: 0.9815\n",
      "Epoch 8/300\n",
      "68864/74147 [==========================>...] - ETA: 0s - loss: 0.2974 - acc: 0.9813Epoch 00008: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.2947 - acc: 0.9815 - val_loss: 0.2939 - val_acc: 0.9816\n",
      "Epoch 9/300\n",
      "70784/74147 [===========================>..] - ETA: 0s - loss: 0.2960 - acc: 0.9815Epoch 00009: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.2941 - acc: 0.9816 - val_loss: 0.2938 - val_acc: 0.9816\n",
      "Epoch 10/300\n",
      "69632/74147 [===========================>..] - ETA: 0s - loss: 0.2955 - acc: 0.9815- ETA: 0s - loss: 0.3013 - acc: 0.9Epoch 00010: val_acc improved from 0.98159 to 0.98171, saving model to bestWeightsMLP1L50.hdf5\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.2937 - acc: 0.9816 - val_loss: 0.2929 - val_acc: 0.9817\n",
      "Epoch 11/300\n",
      "70272/74147 [===========================>..] - ETA: 0s - loss: 0.2972 - acc: 0.9814Epoch 00011: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.2947 - acc: 0.9816 - val_loss: 0.2928 - val_acc: 0.9816\n",
      "Epoch 12/300\n",
      "69504/74147 [===========================>..] - ETA: 0s - loss: 0.2883 - acc: 0.9816Epoch 00012: val_acc improved from 0.98171 to 0.98248, saving model to bestWeightsMLP1L50.hdf5\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.2814 - acc: 0.9815 - val_loss: 0.1425 - val_acc: 0.9825\n",
      "Epoch 13/300\n",
      "71424/74147 [===========================>..] - ETA: 0s - loss: 0.0954 - acc: 0.9894Epoch 00013: val_acc improved from 0.98248 to 0.99251, saving model to bestWeightsMLP1L50.hdf5\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0947 - acc: 0.9896 - val_loss: 0.0823 - val_acc: 0.9925\n",
      "Epoch 14/300\n",
      "70144/74147 [===========================>..] - ETA: 0s - loss: 0.0838 - acc: 0.9916Epoch 00014: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0831 - acc: 0.9917 - val_loss: 0.0829 - val_acc: 0.9917\n",
      "Epoch 15/300\n",
      "73728/74147 [============================>.] - ETA: 0s - loss: 0.0826 - acc: 0.9922Epoch 00015: val_acc improved from 0.99251 to 0.99341, saving model to bestWeightsMLP1L50.hdf5\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0822 - acc: 0.9922 - val_loss: 0.0823 - val_acc: 0.9934\n",
      "Epoch 16/300\n",
      "68864/74147 [==========================>...] - ETA: 0s - loss: 0.0819 - acc: 0.9927Epoch 00016: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0806 - acc: 0.9929 - val_loss: 0.0799 - val_acc: 0.9932\n",
      "Epoch 17/300\n",
      "67456/74147 [==========================>...] - ETA: 0s - loss: 0.0814 - acc: 0.9929Epoch 00017: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0798 - acc: 0.9931 - val_loss: 0.0811 - val_acc: 0.9927\n",
      "Epoch 18/300\n",
      "69376/74147 [===========================>..] - ETA: 0s - loss: 0.0798 - acc: 0.9931- ETA: 0s - loss: 0.0845 - acc: 0.Epoch 00018: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 8us/step - loss: 0.0789 - acc: 0.9932 - val_loss: 0.0812 - val_acc: 0.9927\n",
      "Epoch 19/300\n",
      "72832/74147 [============================>.] - ETA: 0s - loss: 0.0790 - acc: 0.9933Epoch 00019: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 8us/step - loss: 0.0790 - acc: 0.9933 - val_loss: 0.0777 - val_acc: 0.9934\n",
      "Epoch 20/300\n",
      "70656/74147 [===========================>..] - ETA: 0s - loss: 0.0793 - acc: 0.9933Epoch 00020: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 8us/step - loss: 0.0789 - acc: 0.9933 - val_loss: 0.0770 - val_acc: 0.9932\n",
      "Epoch 21/300\n",
      "71808/74147 [============================>.] - ETA: 0s - loss: 0.0811 - acc: 0.9933Epoch 00021: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 8us/step - loss: 0.0804 - acc: 0.9934 - val_loss: 0.0782 - val_acc: 0.9931\n",
      "Epoch 22/300\n",
      "72704/74147 [============================>.] - ETA: 0s - loss: 0.0789 - acc: 0.9935Epoch 00022: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 8us/step - loss: 0.0788 - acc: 0.9935 - val_loss: 0.0779 - val_acc: 0.9930\n",
      "Epoch 23/300\n",
      "68608/74147 [==========================>...] - ETA: 0s - loss: 0.0809 - acc: 0.9932Epoch 00023: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0796 - acc: 0.9934 - val_loss: 0.0800 - val_acc: 0.9929\n",
      "Epoch 24/300\n",
      "72576/74147 [============================>.] - ETA: 0s - loss: 0.0785 - acc: 0.9936Epoch 00024: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 8us/step - loss: 0.0782 - acc: 0.9936 - val_loss: 0.0767 - val_acc: 0.9934\n",
      "Epoch 25/300\n",
      "71552/74147 [===========================>..] - ETA: 0s - loss: 0.0782 - acc: 0.9936- ETA: 0s - loss: 0.0829 - acc: 0.Epoch 00025: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 8us/step - loss: 0.0783 - acc: 0.9936 - val_loss: 0.0774 - val_acc: 0.9931\n",
      "Epoch 26/300\n",
      "72192/74147 [============================>.] - ETA: 0s - loss: 0.0788 - acc: 0.9934Epoch 00026: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 8us/step - loss: 0.0785 - acc: 0.9934 - val_loss: 0.0814 - val_acc: 0.9930\n",
      "Epoch 27/300\n",
      "71424/74147 [===========================>..] - ETA: 0s - loss: 0.0784 - acc: 0.9934Epoch 00027: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 8us/step - loss: 0.0783 - acc: 0.9935 - val_loss: 0.0771 - val_acc: 0.9934\n",
      "Epoch 28/300\n",
      "70272/74147 [===========================>..] - ETA: 0s - loss: 0.0796 - acc: 0.9934Epoch 00028: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 8us/step - loss: 0.0787 - acc: 0.9935 - val_loss: 0.0777 - val_acc: 0.9930\n",
      "Epoch 29/300\n",
      "70912/74147 [===========================>..] - ETA: 0s - loss: 0.0786 - acc: 0.9934Epoch 00029: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 8us/step - loss: 0.0782 - acc: 0.9935 - val_loss: 0.0771 - val_acc: 0.9932\n",
      "Epoch 30/300\n",
      "69888/74147 [===========================>..] - ETA: 0s - loss: 0.0815 - acc: 0.9932Epoch 00030: val_acc improved from 0.99341 to 0.99385, saving model to bestWeightsMLP1L50.hdf5\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0805 - acc: 0.9933 - val_loss: 0.0782 - val_acc: 0.9939\n",
      "Epoch 31/300\n",
      "71936/74147 [============================>.] - ETA: 0s - loss: 0.0813 - acc: 0.9934Epoch 00031: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 8us/step - loss: 0.0810 - acc: 0.9934 - val_loss: 0.0771 - val_acc: 0.9934\n",
      "Epoch 32/300\n",
      "71936/74147 [============================>.] - ETA: 0s - loss: 0.0785 - acc: 0.9935Epoch 00032: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 8us/step - loss: 0.0783 - acc: 0.9935 - val_loss: 0.0762 - val_acc: 0.9932\n",
      "Epoch 33/300\n",
      "71040/74147 [===========================>..] - ETA: 0s - loss: 0.0775 - acc: 0.9937- ETA: 0s - loss: 0.0816 - acc: 0.9Epoch 00033: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 8us/step - loss: 0.0773 - acc: 0.9937 - val_loss: 0.0765 - val_acc: 0.9935\n",
      "Epoch 34/300\n",
      "70912/74147 [===========================>..] - ETA: 0s - loss: 0.0778 - acc: 0.9936Epoch 00034: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 8us/step - loss: 0.0775 - acc: 0.9936 - val_loss: 0.0763 - val_acc: 0.9935\n",
      "Epoch 35/300\n",
      "69120/74147 [==========================>...] - ETA: 0s - loss: 0.0784 - acc: 0.9936Epoch 00035: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0780 - acc: 0.9937 - val_loss: 0.0764 - val_acc: 0.9934\n",
      "Epoch 36/300\n",
      "71552/74147 [===========================>..] - ETA: 0s - loss: 0.0769 - acc: 0.9938Epoch 00036: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 8us/step - loss: 0.0770 - acc: 0.9938 - val_loss: 0.0761 - val_acc: 0.9936\n",
      "Epoch 37/300\n",
      "72576/74147 [============================>.] - ETA: 0s - loss: 0.0778 - acc: 0.9937Epoch 00037: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 8us/step - loss: 0.0776 - acc: 0.9937 - val_loss: 0.0757 - val_acc: 0.9934\n",
      "Epoch 38/300\n",
      "73728/74147 [============================>.] - ETA: 0s - loss: 0.0777 - acc: 0.9936Epoch 00038: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0773 - acc: 0.9936 - val_loss: 0.0761 - val_acc: 0.9934\n",
      "Epoch 39/300\n",
      "70144/74147 [===========================>..] - ETA: 0s - loss: 0.0782 - acc: 0.9935Epoch 00039: val_acc improved from 0.99385 to 0.99401, saving model to bestWeightsMLP1L50.hdf5\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0775 - acc: 0.9936 - val_loss: 0.0756 - val_acc: 0.9940\n",
      "Epoch 40/300\n",
      "69760/74147 [===========================>..] - ETA: 0s - loss: 0.0779 - acc: 0.9936Epoch 00040: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0774 - acc: 0.9937 - val_loss: 0.0764 - val_acc: 0.9935\n",
      "Epoch 41/300\n",
      "71168/74147 [===========================>..] - ETA: 0s - loss: 0.0772 - acc: 0.9938- ETA: 0s - loss: 0.0823 - acc: 0.9 - ETA: 0s - loss: 0.0797 - acc: 0.993Epoch 00041: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0772 - acc: 0.9939 - val_loss: 0.0760 - val_acc: 0.9939\n",
      "Epoch 42/300\n",
      "69120/74147 [==========================>...] - ETA: 0s - loss: 0.0782 - acc: 0.9936Epoch 00042: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0776 - acc: 0.9937 - val_loss: 0.0759 - val_acc: 0.9934\n",
      "Epoch 43/300\n",
      "73216/74147 [============================>.] - ETA: 0s - loss: 0.0767 - acc: 0.9937Epoch 00043: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0769 - acc: 0.9937 - val_loss: 0.0763 - val_acc: 0.9934\n",
      "Epoch 44/300\n",
      "71552/74147 [===========================>..] - ETA: 0s - loss: 0.0797 - acc: 0.993 - ETA: 0s - loss: 0.0769 - acc: 0.9937Epoch 00044: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0770 - acc: 0.9938 - val_loss: 0.0753 - val_acc: 0.9940\n",
      "Epoch 45/300\n",
      "69888/74147 [===========================>..] - ETA: 0s - loss: 0.0775 - acc: 0.9937Epoch 00045: val_acc improved from 0.99401 to 0.99405, saving model to bestWeightsMLP1L50.hdf5\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0768 - acc: 0.9938 - val_loss: 0.0761 - val_acc: 0.9941\n",
      "Epoch 46/300\n",
      "69888/74147 [===========================>..] - ETA: 0s - loss: 0.0783 - acc: 0.9934Epoch 00046: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0777 - acc: 0.9935 - val_loss: 0.0761 - val_acc: 0.9935\n",
      "Epoch 47/300\n",
      "70272/74147 [===========================>..] - ETA: 0s - loss: 0.0778 - acc: 0.9938Epoch 00047: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0771 - acc: 0.9939 - val_loss: 0.0770 - val_acc: 0.9932\n",
      "Epoch 48/300\n",
      "70272/74147 [===========================>..] - ETA: 0s - loss: 0.0777 - acc: 0.9937Epoch 00048: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0770 - acc: 0.9937 - val_loss: 0.0754 - val_acc: 0.9941\n",
      "Epoch 49/300\n",
      "69888/74147 [===========================>..] - ETA: 0s - loss: 0.0775 - acc: 0.9938Epoch 00049: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0770 - acc: 0.9939 - val_loss: 0.0761 - val_acc: 0.9937\n",
      "Epoch 50/300\n",
      "70912/74147 [===========================>..] - ETA: 0s - loss: 0.0793 - acc: 0.9936Epoch 00050: val_acc improved from 0.99405 to 0.99413, saving model to bestWeightsMLP1L50.hdf5\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0790 - acc: 0.9936 - val_loss: 0.0759 - val_acc: 0.9941\n",
      "Epoch 51/300\n",
      "70400/74147 [===========================>..] - ETA: 0s - loss: 0.0774 - acc: 0.9937000 - ETA: 0s - loss: 0.0759 - acc: Epoch 00051: val_acc improved from 0.99413 to 0.99421, saving model to bestWeightsMLP1L50.hdf5\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0769 - acc: 0.9937 - val_loss: 0.0747 - val_acc: 0.9942\n",
      "Epoch 52/300\n",
      "69632/74147 [===========================>..] - ETA: 0s - loss: 0.0773 - acc: 0.9937Epoch 00052: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0768 - acc: 0.9938 - val_loss: 0.0753 - val_acc: 0.9939\n",
      "Epoch 53/300\n",
      "70144/74147 [===========================>..] - ETA: 0s - loss: 0.0776 - acc: 0.9936Epoch 00053: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0769 - acc: 0.9937 - val_loss: 0.0749 - val_acc: 0.9941\n",
      "Epoch 54/300\n",
      "71040/74147 [===========================>..] - ETA: 0s - loss: 0.0794 - acc: 0.993 - ETA: 0s - loss: 0.0772 - acc: 0.9937Epoch 00054: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0771 - acc: 0.9938 - val_loss: 0.0751 - val_acc: 0.9941\n",
      "Epoch 55/300\n",
      "70784/74147 [===========================>..] - ETA: 0s - loss: 0.0768 - acc: 0.9938Epoch 00055: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0764 - acc: 0.9938 - val_loss: 0.0838 - val_acc: 0.9925\n",
      "Epoch 56/300\n",
      "70272/74147 [===========================>..] - ETA: 0s - loss: 0.0783 - acc: 0.9937Epoch 00056: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0777 - acc: 0.9937 - val_loss: 0.0762 - val_acc: 0.9934\n",
      "Epoch 57/300\n",
      "71424/74147 [===========================>..] - ETA: 0s - loss: 0.0772 - acc: 0.9936- ETA: 0s - loss: 0.0797 - acc: 0.9Epoch 00057: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0771 - acc: 0.9936 - val_loss: 0.0752 - val_acc: 0.9939\n",
      "Epoch 58/300\n",
      "70400/74147 [===========================>..] - ETA: 0s - loss: 0.0778 - acc: 0.9937Epoch 00058: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0773 - acc: 0.9937 - val_loss: 0.0766 - val_acc: 0.9935\n",
      "Epoch 59/300\n",
      "70784/74147 [===========================>..] - ETA: 0s - loss: 0.0775 - acc: 0.9938Epoch 00059: val_acc improved from 0.99421 to 0.99438, saving model to bestWeightsMLP1L50.hdf5\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0770 - acc: 0.9938 - val_loss: 0.0743 - val_acc: 0.9944\n",
      "Epoch 60/300\n",
      "70912/74147 [===========================>..] - ETA: 0s - loss: 0.0768 - acc: 0.9938Epoch 00060: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0764 - acc: 0.9938 - val_loss: 0.0763 - val_acc: 0.9939\n",
      "Epoch 61/300\n",
      "69376/74147 [===========================>..] - ETA: 0s - loss: 0.0775 - acc: 0.9936Epoch 00061: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0769 - acc: 0.9937 - val_loss: 0.0758 - val_acc: 0.9941\n",
      "Epoch 62/300\n",
      "70016/74147 [===========================>..] - ETA: 0s - loss: 0.0776 - acc: 0.9937Epoch 00062: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0770 - acc: 0.9938 - val_loss: 0.0754 - val_acc: 0.9941\n",
      "Epoch 63/300\n",
      "70912/74147 [===========================>..] - ETA: 0s - loss: 0.0773 - acc: 0.9938Epoch 00063: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0770 - acc: 0.9938 - val_loss: 0.0755 - val_acc: 0.9938\n",
      "Epoch 64/300\n",
      "70784/74147 [===========================>..] - ETA: 0s - loss: 0.0774 - acc: 0.9938Epoch 00064: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0769 - acc: 0.9938 - val_loss: 0.0776 - val_acc: 0.9940\n",
      "Epoch 65/300\n",
      "70400/74147 [===========================>..] - ETA: 0s - loss: 0.0770 - acc: 0.9937Epoch 00065: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0764 - acc: 0.9937 - val_loss: 0.0756 - val_acc: 0.9939\n",
      "Epoch 66/300\n",
      "70528/74147 [===========================>..] - ETA: 0s - loss: 0.0776 - acc: 0.9936- ETA: 0s - loss: 0.0810 - acc: 0.Epoch 00066: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0771 - acc: 0.9937 - val_loss: 0.0755 - val_acc: 0.9941\n",
      "Epoch 67/300\n",
      "70784/74147 [===========================>..] - ETA: 0s - loss: 0.0771 - acc: 0.9938Epoch 00067: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0766 - acc: 0.9939 - val_loss: 0.0753 - val_acc: 0.9942\n",
      "Epoch 68/300\n",
      "68736/74147 [==========================>...] - ETA: 0s - loss: 0.0773 - acc: 0.9938- ETA: 0s - loss: 0.0789 - acc: 0.99Epoch 00068: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0765 - acc: 0.9939 - val_loss: 0.0754 - val_acc: 0.9941\n",
      "Epoch 69/300\n",
      "67840/74147 [==========================>...] - ETA: 0s - loss: 0.0780 - acc: 0.9937Epoch 00069: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0771 - acc: 0.9938 - val_loss: 0.0760 - val_acc: 0.9936\n",
      "Epoch 70/300\n",
      "68864/74147 [==========================>...] - ETA: 0s - loss: 0.0776 - acc: 0.9937Epoch 00070: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0767 - acc: 0.9938 - val_loss: 0.0752 - val_acc: 0.9941\n",
      "Epoch 71/300\n",
      "69120/74147 [==========================>...] - ETA: 0s - loss: 0.0776 - acc: 0.9936Epoch 00071: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0770 - acc: 0.9936 - val_loss: 0.0764 - val_acc: 0.9938\n",
      "Epoch 72/300\n",
      "73216/74147 [============================>.] - ETA: 0s - loss: 0.0776 - acc: 0.9938Epoch 00072: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0777 - acc: 0.9937 - val_loss: 0.0762 - val_acc: 0.9939\n",
      "Epoch 73/300\n",
      "70272/74147 [===========================>..] - ETA: 0s - loss: 0.0775 - acc: 0.9936Epoch 00073: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0767 - acc: 0.9937 - val_loss: 0.0786 - val_acc: 0.9936\n",
      "Epoch 74/300\n",
      "70656/74147 [===========================>..] - ETA: 0s - loss: 0.0781 - acc: 0.9935Epoch 00074: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0777 - acc: 0.9936 - val_loss: 0.0758 - val_acc: 0.9940\n",
      "Epoch 75/300\n",
      "70656/74147 [===========================>..] - ETA: 0s - loss: 0.0774 - acc: 0.9937Epoch 00075: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0773 - acc: 0.9937 - val_loss: 0.0792 - val_acc: 0.9929\n",
      "Epoch 76/300\n",
      "70912/74147 [===========================>..] - ETA: 0s - loss: 0.0795 - acc: 0.9935Epoch 00076: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0790 - acc: 0.9936 - val_loss: 0.0755 - val_acc: 0.9943\n",
      "Epoch 77/300\n",
      "70912/74147 [===========================>..] - ETA: 0s - loss: 0.0775 - acc: 0.9937Epoch 00077: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0773 - acc: 0.9938 - val_loss: 0.0751 - val_acc: 0.9938\n",
      "Epoch 78/300\n",
      "70272/74147 [===========================>..] - ETA: 0s - loss: 0.0773 - acc: 0.9938Epoch 00078: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0766 - acc: 0.9939 - val_loss: 0.0764 - val_acc: 0.9939\n",
      "Epoch 79/300\n",
      "70400/74147 [===========================>..] - ETA: 0s - loss: 0.0791 - acc: 0.9934- ETA: 0s - loss: 0.0814 - acc: 0.99Epoch 00079: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0784 - acc: 0.9935 - val_loss: 0.0765 - val_acc: 0.9936\n",
      "Epoch 80/300\n",
      "70912/74147 [===========================>..] - ETA: 0s - loss: 0.0776 - acc: 0.9936Epoch 00080: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0772 - acc: 0.9937 - val_loss: 0.0760 - val_acc: 0.9939\n",
      "Epoch 81/300\n",
      "70144/74147 [===========================>..] - ETA: 0s - loss: 0.0776 - acc: 0.9937Epoch 00081: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0769 - acc: 0.9937 - val_loss: 0.0765 - val_acc: 0.9941\n",
      "Epoch 82/300\n",
      "70144/74147 [===========================>..] - ETA: 0s - loss: 0.0778 - acc: 0.9937Epoch 00082: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0771 - acc: 0.9938 - val_loss: 0.0750 - val_acc: 0.9943\n",
      "Epoch 83/300\n",
      "71424/74147 [===========================>..] - ETA: 0s - loss: 0.0763 - acc: 0.9938Epoch 00083: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0762 - acc: 0.9938 - val_loss: 0.0751 - val_acc: 0.9943\n",
      "Epoch 84/300\n",
      "70144/74147 [===========================>..] - ETA: 0s - loss: 0.0779 - acc: 0.9938- ETA: 0s - loss: 0.0807 - acc: 0.9Epoch 00084: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0771 - acc: 0.9938 - val_loss: 0.0750 - val_acc: 0.9942\n",
      "Epoch 85/300\n",
      "70784/74147 [===========================>..] - ETA: 0s - loss: 0.0772 - acc: 0.9938Epoch 00085: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0767 - acc: 0.9938 - val_loss: 0.0760 - val_acc: 0.9941\n",
      "Epoch 86/300\n",
      "71808/74147 [============================>.] - ETA: 0s - loss: 0.0766 - acc: 0.9939Epoch 00086: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0763 - acc: 0.9940 - val_loss: 0.0748 - val_acc: 0.9944\n",
      "Epoch 87/300\n",
      "70784/74147 [===========================>..] - ETA: 0s - loss: 0.0770 - acc: 0.9938Epoch 00087: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0765 - acc: 0.9938 - val_loss: 0.0748 - val_acc: 0.9944\n",
      "Epoch 88/300\n",
      "70400/74147 [===========================>..] - ETA: 0s - loss: 0.0781 - acc: 0.9935Epoch 00088: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0774 - acc: 0.9936 - val_loss: 0.0748 - val_acc: 0.9943\n",
      "Epoch 89/300\n",
      "71040/74147 [===========================>..] - ETA: 0s - loss: 0.0765 - acc: 0.9938Epoch 00089: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0763 - acc: 0.9939 - val_loss: 0.0747 - val_acc: 0.9943\n",
      "Epoch 90/300\n",
      "71296/74147 [===========================>..] - ETA: 0s - loss: 0.0765 - acc: 0.9938 - ETA: 0s - loss: 0.0761 - acc: Epoch 00090: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0765 - acc: 0.9938 - val_loss: 0.0746 - val_acc: 0.9942\n",
      "Epoch 91/300\n",
      "70272/74147 [===========================>..] - ETA: 0s - loss: 0.0857 - acc: 0.9932Epoch 00091: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0844 - acc: 0.9933 - val_loss: 0.0754 - val_acc: 0.9943\n",
      "Epoch 92/300\n",
      "71296/74147 [===========================>..] - ETA: 0s - loss: 0.0776 - acc: 0.9938Epoch 00092: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0778 - acc: 0.9938 - val_loss: 0.0804 - val_acc: 0.9924\n",
      "Epoch 93/300\n",
      "71040/74147 [===========================>..] - ETA: 0s - loss: 0.0782 - acc: 0.9934Epoch 00093: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0780 - acc: 0.9935 - val_loss: 0.0752 - val_acc: 0.9943\n",
      "Epoch 94/300\n",
      "70400/74147 [===========================>..] - ETA: 0s - loss: 0.0772 - acc: 0.9939Epoch 00094: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0765 - acc: 0.9940 - val_loss: 0.0751 - val_acc: 0.9943\n",
      "Epoch 95/300\n",
      "70784/74147 [===========================>..] - ETA: 0s - loss: 0.0779 - acc: 0.9938Epoch 00095: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0775 - acc: 0.9938 - val_loss: 0.0794 - val_acc: 0.9923\n",
      "Epoch 96/300\n",
      "71296/74147 [===========================>..] - ETA: 0s - loss: 0.0790 - acc: 0.9932Epoch 00096: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0789 - acc: 0.9933 - val_loss: 0.0769 - val_acc: 0.9937\n",
      "Epoch 97/300\n",
      "70912/74147 [===========================>..] - ETA: 0s - loss: 0.0779 - acc: 0.9937Epoch 00097: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0775 - acc: 0.9938 - val_loss: 0.0761 - val_acc: 0.9940\n",
      "Epoch 98/300\n",
      "69376/74147 [===========================>..] - ETA: 0s - loss: 0.0779 - acc: 0.9938Epoch 00098: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0773 - acc: 0.9939 - val_loss: 0.0799 - val_acc: 0.9936\n",
      "Epoch 99/300\n",
      "70528/74147 [===========================>..] - ETA: 0s - loss: 0.0778 - acc: 0.9938- ETA: 0s - loss: 0.0800 - acc: 0.9Epoch 00099: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0773 - acc: 0.9939 - val_loss: 0.0756 - val_acc: 0.9940\n",
      "Epoch 100/300\n",
      "68352/74147 [==========================>...] - ETA: 0s - loss: 0.0781 - acc: 0.9938Epoch 00100: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0771 - acc: 0.9939 - val_loss: 0.0754 - val_acc: 0.9941\n",
      "Epoch 101/300\n",
      "69888/74147 [===========================>..] - ETA: 0s - loss: 0.0779 - acc: 0.9938Epoch 00101: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0771 - acc: 0.9939 - val_loss: 0.0759 - val_acc: 0.9937\n",
      "Epoch 102/300\n",
      "70528/74147 [===========================>..] - ETA: 0s - loss: 0.0785 - acc: 0.9937Epoch 00102: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0779 - acc: 0.9938 - val_loss: 0.0798 - val_acc: 0.9927\n",
      "Epoch 103/300\n",
      "70528/74147 [===========================>..] - ETA: 0s - loss: 0.0780 - acc: 0.9938Epoch 00103: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0775 - acc: 0.9939 - val_loss: 0.0757 - val_acc: 0.9942\n",
      "Epoch 104/300\n",
      "70016/74147 [===========================>..] - ETA: 0s - loss: 0.0779 - acc: 0.9938Epoch 00104: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0770 - acc: 0.9939 - val_loss: 0.0755 - val_acc: 0.9942\n",
      "Epoch 105/300\n",
      "70912/74147 [===========================>..] - ETA: 0s - loss: 0.0777 - acc: 0.9937- ETA: 0s - loss: 0.0808 - acc: 0Epoch 00105: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0773 - acc: 0.9938 - val_loss: 0.0757 - val_acc: 0.9943\n",
      "Epoch 106/300\n",
      "70400/74147 [===========================>..] - ETA: 0s - loss: 0.0777 - acc: 0.9938Epoch 00106: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0770 - acc: 0.9939 - val_loss: 0.0760 - val_acc: 0.9944\n",
      "Epoch 107/300\n",
      "70144/74147 [===========================>..] - ETA: 0s - loss: 0.0792 - acc: 0.9937Epoch 00107: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0787 - acc: 0.9937 - val_loss: 0.0806 - val_acc: 0.9924\n",
      "Epoch 108/300\n",
      "70912/74147 [===========================>..] - ETA: 0s - loss: 0.0788 - acc: 0.9938Epoch 00108: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0783 - acc: 0.9939 - val_loss: 0.0762 - val_acc: 0.9943\n",
      "Epoch 109/300\n",
      "71296/74147 [===========================>..] - ETA: 0s - loss: 0.0777 - acc: 0.9938Epoch 00109: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0779 - acc: 0.9938 - val_loss: 0.0775 - val_acc: 0.9932\n",
      "Epoch 110/300\n",
      "70016/74147 [===========================>..] - ETA: 0s - loss: 0.0783 - acc: 0.9937Epoch 00110: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0774 - acc: 0.9938 - val_loss: 0.0771 - val_acc: 0.9937\n",
      "Epoch 111/300\n",
      "69632/74147 [===========================>..] - ETA: 0s - loss: 0.0777 - acc: 0.9938Epoch 00111: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0770 - acc: 0.9939 - val_loss: 0.0762 - val_acc: 0.9943\n",
      "Epoch 112/300\n",
      "68608/74147 [==========================>...] - ETA: 0s - loss: 0.0778 - acc: 0.9938Epoch 00112: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0771 - acc: 0.9939 - val_loss: 0.0780 - val_acc: 0.9931\n",
      "Epoch 113/300\n",
      "69248/74147 [===========================>..] - ETA: 0s - loss: 0.0780 - acc: 0.9938- ETA: 0s - loss: 0.0800 - acc: 0.99 - ETA: 0s - loss: 0.0817 - acc: 0.9Epoch 00113: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0774 - acc: 0.9939 - val_loss: 0.0758 - val_acc: 0.9944\n",
      "Epoch 114/300\n",
      "68608/74147 [==========================>...] - ETA: 0s - loss: 0.0779 - acc: 0.9938Epoch 00114: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0772 - acc: 0.9938 - val_loss: 0.0801 - val_acc: 0.9925\n",
      "Epoch 115/300\n",
      "71168/74147 [===========================>..] - ETA: 0s - loss: 0.0780 - acc: 0.9936Epoch 00115: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0778 - acc: 0.9937 - val_loss: 0.0762 - val_acc: 0.9942\n",
      "Epoch 116/300\n",
      "69632/74147 [===========================>..] - ETA: 0s - loss: 0.0776 - acc: 0.9938Epoch 00116: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0773 - acc: 0.9938 - val_loss: 0.0800 - val_acc: 0.9926\n",
      "Epoch 117/300\n",
      "70784/74147 [===========================>..] - ETA: 0s - loss: 0.0784 - acc: 0.9937Epoch 00117: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0778 - acc: 0.9937 - val_loss: 0.0770 - val_acc: 0.9942\n",
      "Epoch 118/300\n",
      "70784/74147 [===========================>..] - ETA: 0s - loss: 0.0779 - acc: 0.9938Epoch 00118: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0774 - acc: 0.9938 - val_loss: 0.0764 - val_acc: 0.9943\n",
      "Epoch 119/300\n",
      "70400/74147 [===========================>..] - ETA: 0s - loss: 0.0774 - acc: 0.9939Epoch 00119: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0767 - acc: 0.9940 - val_loss: 0.0759 - val_acc: 0.9942\n",
      "Epoch 120/300\n",
      "70272/74147 [===========================>..] - ETA: 0s - loss: 0.0787 - acc: 0.9936Epoch 00120: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0779 - acc: 0.9936 - val_loss: 0.0768 - val_acc: 0.9936\n",
      "Epoch 121/300\n",
      "70016/74147 [===========================>..] - ETA: 0s - loss: 0.0789 - acc: 0.9936Epoch 00121: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0779 - acc: 0.9937 - val_loss: 0.0776 - val_acc: 0.9939\n",
      "Epoch 122/300\n",
      "70656/74147 [===========================>..] - ETA: 0s - loss: 0.0779 - acc: 0.9939Epoch 00122: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0774 - acc: 0.9939 - val_loss: 0.0773 - val_acc: 0.9940\n",
      "Epoch 123/300\n",
      "69888/74147 [===========================>..] - ETA: 0s - loss: 0.0782 - acc: 0.9939Epoch 00123: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0774 - acc: 0.9940 - val_loss: 0.0766 - val_acc: 0.9944\n",
      "Epoch 124/300\n",
      "70144/74147 [===========================>..] - ETA: 0s - loss: 0.0783 - acc: 0.9938Epoch 00124: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0774 - acc: 0.9939 - val_loss: 0.0764 - val_acc: 0.9943\n",
      "Epoch 125/300\n",
      "70912/74147 [===========================>..] - ETA: 0s - loss: 0.0779 - acc: 0.9938Epoch 00125: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0774 - acc: 0.9939 - val_loss: 0.0761 - val_acc: 0.9944\n",
      "Epoch 126/300\n",
      "70272/74147 [===========================>..] - ETA: 0s - loss: 0.0791 - acc: 0.9937Epoch 00126: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0782 - acc: 0.9938 - val_loss: 0.0777 - val_acc: 0.9939\n",
      "Epoch 127/300\n",
      "70400/74147 [===========================>..] - ETA: 0s - loss: 0.0779 - acc: 0.9938Epoch 00127: val_acc improved from 0.99438 to 0.99442, saving model to bestWeightsMLP1L50.hdf5\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0771 - acc: 0.9939 - val_loss: 0.0758 - val_acc: 0.9944\n",
      "Epoch 128/300\n",
      "71424/74147 [===========================>..] - ETA: 0s - loss: 0.0774 - acc: 0.9939Epoch 00128: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0773 - acc: 0.9940 - val_loss: 0.0776 - val_acc: 0.9939\n",
      "Epoch 129/300\n",
      "70016/74147 [===========================>..] - ETA: 0s - loss: 0.0786 - acc: 0.9937Epoch 00129: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0778 - acc: 0.9938 - val_loss: 0.0798 - val_acc: 0.9935\n",
      "Epoch 130/300\n",
      "71168/74147 [===========================>..] - ETA: 0s - loss: 0.0779 - acc: 0.9938Epoch 00130: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0777 - acc: 0.9938 - val_loss: 0.0760 - val_acc: 0.9943\n",
      "Epoch 131/300\n",
      "70912/74147 [===========================>..] - ETA: 0s - loss: 0.0781 - acc: 0.9939Epoch 00131: val_acc improved from 0.99442 to 0.99446, saving model to bestWeightsMLP1L50.hdf5\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0776 - acc: 0.9940 - val_loss: 0.0757 - val_acc: 0.9945\n",
      "Epoch 132/300\n",
      "70784/74147 [===========================>..] - ETA: 0s - loss: 0.0779 - acc: 0.9939Epoch 00132: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0773 - acc: 0.9940 - val_loss: 0.0760 - val_acc: 0.9944\n",
      "Epoch 133/300\n",
      "70016/74147 [===========================>..] - ETA: 0s - loss: 0.0784 - acc: 0.9939Epoch 00133: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0774 - acc: 0.9940 - val_loss: 0.0759 - val_acc: 0.9944\n",
      "Epoch 134/300\n",
      "70144/74147 [===========================>..] - ETA: 0s - loss: 0.0779 - acc: 0.9940Epoch 00134: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0771 - acc: 0.9941 - val_loss: 0.0765 - val_acc: 0.9943\n",
      "Epoch 135/300\n",
      "71040/74147 [===========================>..] - ETA: 0s - loss: 0.0775 - acc: 0.9939Epoch 00135: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0771 - acc: 0.9940 - val_loss: 0.0761 - val_acc: 0.9943\n",
      "Epoch 136/300\n",
      "69760/74147 [===========================>..] - ETA: 0s - loss: 0.0779 - acc: 0.9940Epoch 00136: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0774 - acc: 0.9940 - val_loss: 0.0801 - val_acc: 0.9934\n",
      "Epoch 137/300\n",
      "68480/74147 [==========================>...] - ETA: 0s - loss: 0.0786 - acc: 0.9938- ETA: 0s - loss: 0.0805 - acc: 0.993Epoch 00137: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0774 - acc: 0.9939 - val_loss: 0.0773 - val_acc: 0.9941\n",
      "Epoch 138/300\n",
      "70528/74147 [===========================>..] - ETA: 0s - loss: 0.0775 - acc: 0.9939Epoch 00138: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0769 - acc: 0.9940 - val_loss: 0.0763 - val_acc: 0.9943\n",
      "Epoch 139/300\n",
      "70144/74147 [===========================>..] - ETA: 0s - loss: 0.0779 - acc: 0.9939Epoch 00139: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0770 - acc: 0.9940 - val_loss: 0.0765 - val_acc: 0.9943\n",
      "Epoch 140/300\n",
      "70016/74147 [===========================>..] - ETA: 0s - loss: 0.0780 - acc: 0.9939Epoch 00140: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0771 - acc: 0.9940 - val_loss: 0.0770 - val_acc: 0.9942\n",
      "Epoch 141/300\n",
      "70528/74147 [===========================>..] - ETA: 0s - loss: 0.0777 - acc: 0.9939Epoch 00141: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0773 - acc: 0.9939 - val_loss: 0.0800 - val_acc: 0.9937\n",
      "Epoch 142/300\n",
      "70016/74147 [===========================>..] - ETA: 0s - loss: 0.0790 - acc: 0.9938Epoch 00142: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0779 - acc: 0.9939 - val_loss: 0.0759 - val_acc: 0.9942\n",
      "Epoch 143/300\n",
      "69632/74147 [===========================>..] - ETA: 0s - loss: 0.0781 - acc: 0.9938Epoch 00143: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0774 - acc: 0.9939 - val_loss: 0.0767 - val_acc: 0.9942\n",
      "Epoch 144/300\n",
      "70656/74147 [===========================>..] - ETA: 0s - loss: 0.0775 - acc: 0.9938Epoch 00144: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0773 - acc: 0.9939 - val_loss: 0.0778 - val_acc: 0.9938\n",
      "Epoch 145/300\n",
      "74112/74147 [============================>.] - ETA: 0s - loss: 0.0765 - acc: 0.9939Epoch 00145: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0764 - acc: 0.9939 - val_loss: 0.0755 - val_acc: 0.9943\n",
      "Epoch 146/300\n",
      "70144/74147 [===========================>..] - ETA: 0s - loss: 0.0771 - acc: 0.9939Epoch 00146: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0763 - acc: 0.9940 - val_loss: 0.0755 - val_acc: 0.9943\n",
      "Epoch 147/300\n",
      "73344/74147 [============================>.] - ETA: 0s - loss: 0.0768 - acc: 0.9940Epoch 00147: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0765 - acc: 0.9940 - val_loss: 0.0752 - val_acc: 0.9944\n",
      "Epoch 148/300\n",
      "73984/74147 [============================>.] - ETA: 0s - loss: 0.0762 - acc: 0.9940- ETA: 0s - loss: 0.0781 - acc: 0.9Epoch 00148: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0761 - acc: 0.9941 - val_loss: 0.0753 - val_acc: 0.9943\n",
      "Epoch 149/300\n",
      "73088/74147 [============================>.] - ETA: 0s - loss: 0.0761 - acc: 0.9940- ETA: 0s - loss: 0.0792 - acc: 0.99Epoch 00149: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0762 - acc: 0.9940 - val_loss: 0.0786 - val_acc: 0.9936\n",
      "Epoch 150/300\n",
      "72192/74147 [============================>.] - ETA: 0s - loss: 0.0766 - acc: 0.9939Epoch 00150: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0764 - acc: 0.9939 - val_loss: 0.0750 - val_acc: 0.9943\n",
      "Epoch 151/300\n",
      "73216/74147 [============================>.] - ETA: 0s - loss: 0.0767 - acc: 0.9938Epoch 00151: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0770 - acc: 0.9938 - val_loss: 0.0756 - val_acc: 0.9940\n",
      "Epoch 152/300\n",
      "72320/74147 [============================>.] - ETA: 0s - loss: 0.0769 - acc: 0.9940Epoch 00152: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0764 - acc: 0.9940 - val_loss: 0.0754 - val_acc: 0.9942\n",
      "Epoch 153/300\n",
      "72448/74147 [============================>.] - ETA: 0s - loss: 0.0765 - acc: 0.9940Epoch 00153: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0763 - acc: 0.9940 - val_loss: 0.0787 - val_acc: 0.9940\n",
      "Epoch 154/300\n",
      "71424/74147 [===========================>..] - ETA: 0s - loss: 0.0769 - acc: 0.9938Epoch 00154: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0768 - acc: 0.9938 - val_loss: 0.0757 - val_acc: 0.9943\n",
      "Epoch 155/300\n",
      "72960/74147 [============================>.] - ETA: 0s - loss: 0.0763 - acc: 0.9940Epoch 00155: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0763 - acc: 0.9940 - val_loss: 0.0759 - val_acc: 0.9942\n",
      "Epoch 156/300\n",
      "72064/74147 [============================>.] - ETA: 0s - loss: 0.0766 - acc: 0.9939Epoch 00156: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0763 - acc: 0.9939 - val_loss: 0.0763 - val_acc: 0.9943\n",
      "Epoch 157/300\n",
      "72192/74147 [============================>.] - ETA: 0s - loss: 0.0764 - acc: 0.9940Epoch 00157: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 11us/step - loss: 0.0762 - acc: 0.9941 - val_loss: 0.0754 - val_acc: 0.9944\n",
      "Epoch 158/300\n",
      "73216/74147 [============================>.] - ETA: 0s - loss: 0.0766 - acc: 0.9939Epoch 00158: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0770 - acc: 0.9939 - val_loss: 0.0804 - val_acc: 0.9934\n",
      "Epoch 159/300\n",
      "73984/74147 [============================>.] - ETA: 0s - loss: 0.0773 - acc: 0.9936Epoch 00159: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0771 - acc: 0.9936 - val_loss: 0.0757 - val_acc: 0.9942\n",
      "Epoch 160/300\n",
      "68352/74147 [==========================>...] - ETA: 0s - loss: 0.0777 - acc: 0.9938Epoch 00160: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0766 - acc: 0.9939 - val_loss: 0.0757 - val_acc: 0.9944\n",
      "Epoch 161/300\n",
      "74112/74147 [============================>.] - ETA: 0s - loss: 0.0776 - acc: 0.9938Epoch 00161: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0775 - acc: 0.9938 - val_loss: 0.0757 - val_acc: 0.9943\n",
      "Epoch 162/300\n",
      "73600/74147 [============================>.] - ETA: 0s - loss: 0.0776 - acc: 0.9938Epoch 00162: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0773 - acc: 0.9939 - val_loss: 0.0760 - val_acc: 0.9942\n",
      "Epoch 163/300\n",
      "73600/74147 [============================>.] - ETA: 0s - loss: 0.0765 - acc: 0.9940Epoch 00163: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0762 - acc: 0.9941 - val_loss: 0.0758 - val_acc: 0.9943\n",
      "Epoch 164/300\n",
      "71680/74147 [============================>.] - ETA: 0s - loss: 0.0768 - acc: 0.9939Epoch 00164: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0762 - acc: 0.9940 - val_loss: 0.0754 - val_acc: 0.9943\n",
      "Epoch 165/300\n",
      "71680/74147 [============================>.] - ETA: 0s - loss: 0.0777 - acc: 0.9936Epoch 00165: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0771 - acc: 0.9937 - val_loss: 0.0753 - val_acc: 0.9944\n",
      "Epoch 166/300\n",
      "73216/74147 [============================>.] - ETA: 0s - loss: 0.0763 - acc: 0.9940- ETA: 0s - loss: 0.0798 - acc: 0.99Epoch 00166: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0765 - acc: 0.9940 - val_loss: 0.0765 - val_acc: 0.9941\n",
      "Epoch 167/300\n",
      "73216/74147 [============================>.] - ETA: 0s - loss: 0.0766 - acc: 0.9940Epoch 00167: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0769 - acc: 0.9940 - val_loss: 0.0774 - val_acc: 0.9940\n",
      "Epoch 168/300\n",
      "71424/74147 [===========================>..] - ETA: 0s - loss: 0.0766 - acc: 0.9939Epoch 00168: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0768 - acc: 0.9939 - val_loss: 0.0772 - val_acc: 0.9941\n",
      "Epoch 169/300\n",
      "72704/74147 [============================>.] - ETA: 0s - loss: 0.0774 - acc: 0.9939Epoch 00169: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0774 - acc: 0.9939 - val_loss: 0.0777 - val_acc: 0.9938\n",
      "Epoch 170/300\n",
      "71296/74147 [===========================>..] - ETA: 0s - loss: 0.0768 - acc: 0.9939- ETA: 0s - loss: 0.0810 - acc: 0.Epoch 00170: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0769 - acc: 0.9939 - val_loss: 0.0764 - val_acc: 0.9943\n",
      "Epoch 171/300\n",
      "72832/74147 [============================>.] - ETA: 0s - loss: 0.0761 - acc: 0.9940Epoch 00171: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0762 - acc: 0.9940 - val_loss: 0.0774 - val_acc: 0.9937\n",
      "Epoch 172/300\n",
      "73472/74147 [============================>.] - ETA: 0s - loss: 0.0769 - acc: 0.9939Epoch 00172: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0765 - acc: 0.9939 - val_loss: 0.0760 - val_acc: 0.9939\n",
      "Epoch 173/300\n",
      "72704/74147 [============================>.] - ETA: 0s - loss: 0.0762 - acc: 0.9941- ETA: 0s - loss: 0.0788 - acc: 0.9Epoch 00173: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0761 - acc: 0.9941 - val_loss: 0.0757 - val_acc: 0.9941\n",
      "Epoch 174/300\n",
      "73088/74147 [============================>.] - ETA: 0s - loss: 0.0760 - acc: 0.9940Epoch 00174: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0760 - acc: 0.9940 - val_loss: 0.0758 - val_acc: 0.9942\n",
      "Epoch 175/300\n",
      "73600/74147 [============================>.] - ETA: 0s - loss: 0.0768 - acc: 0.9940Epoch 00175: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0766 - acc: 0.9940 - val_loss: 0.0791 - val_acc: 0.9939\n",
      "Epoch 176/300\n",
      "71808/74147 [============================>.] - ETA: 0s - loss: 0.0772 - acc: 0.9939Epoch 00176: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0768 - acc: 0.9939 - val_loss: 0.0749 - val_acc: 0.9944\n",
      "Epoch 177/300\n",
      "71936/74147 [============================>.] - ETA: 0s - loss: 0.0767 - acc: 0.9939Epoch 00177: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0764 - acc: 0.9940 - val_loss: 0.0747 - val_acc: 0.9944\n",
      "Epoch 178/300\n",
      "72320/74147 [============================>.] - ETA: 0s - loss: 0.0772 - acc: 0.9939Epoch 00178: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0767 - acc: 0.9940 - val_loss: 0.0753 - val_acc: 0.9944\n",
      "Epoch 179/300\n",
      "73216/74147 [============================>.] - ETA: 0s - loss: 0.0774 - acc: 0.9938Epoch 00179: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0775 - acc: 0.9938 - val_loss: 0.0759 - val_acc: 0.9943\n",
      "Epoch 180/300\n",
      "73216/74147 [============================>.] - ETA: 0s - loss: 0.0761 - acc: 0.9941Epoch 00180: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0763 - acc: 0.9941 - val_loss: 0.0760 - val_acc: 0.9941\n",
      "Epoch 181/300\n",
      "73856/74147 [============================>.] - ETA: 0s - loss: 0.0776 - acc: 0.9939Epoch 00181: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0773 - acc: 0.9939 - val_loss: 0.0761 - val_acc: 0.9942\n",
      "Epoch 182/300\n",
      "72576/74147 [============================>.] - ETA: 0s - loss: 0.0779 - acc: 0.9938Epoch 00182: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0776 - acc: 0.9939 - val_loss: 0.0759 - val_acc: 0.9944\n",
      "Epoch 183/300\n",
      "73216/74147 [============================>.] - ETA: 0s - loss: 0.0760 - acc: 0.9940Epoch 00183: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0764 - acc: 0.9940 - val_loss: 0.0760 - val_acc: 0.9945\n",
      "Epoch 184/300\n",
      "72704/74147 [============================>.] - ETA: 0s - loss: 0.0765 - acc: 0.9939Epoch 00184: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0764 - acc: 0.9939 - val_loss: 0.0767 - val_acc: 0.9942\n",
      "Epoch 185/300\n",
      "69760/74147 [===========================>..] - ETA: 0s - loss: 0.0767 - acc: 0.9939Epoch 00185: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0762 - acc: 0.9940 - val_loss: 0.0754 - val_acc: 0.9941\n",
      "Epoch 186/300\n",
      "71296/74147 [===========================>..] - ETA: 0s - loss: 0.0765 - acc: 0.9939Epoch 00186: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0766 - acc: 0.9940 - val_loss: 0.0760 - val_acc: 0.9941\n",
      "Epoch 187/300\n",
      "71296/74147 [===========================>..] - ETA: 0s - loss: 0.0761 - acc: 0.9940Epoch 00187: val_acc improved from 0.99446 to 0.99450, saving model to bestWeightsMLP1L50.hdf5\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0760 - acc: 0.9940 - val_loss: 0.0755 - val_acc: 0.9945\n",
      "Epoch 188/300\n",
      "70144/74147 [===========================>..] - ETA: 0s - loss: 0.0769 - acc: 0.9940 - ETA: 0s - loss: 0.0760 - acc:Epoch 00188: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0762 - acc: 0.9940 - val_loss: 0.0765 - val_acc: 0.9941\n",
      "Epoch 189/300\n",
      "71168/74147 [===========================>..] - ETA: 0s - loss: 0.0773 - acc: 0.9937- ETA: 0s - loss: 0.0821 - acc: 0Epoch 00189: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0772 - acc: 0.9938 - val_loss: 0.0763 - val_acc: 0.9941\n",
      "Epoch 190/300\n",
      "70912/74147 [===========================>..] - ETA: 0s - loss: 0.0772 - acc: 0.9938Epoch 00190: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0768 - acc: 0.9939 - val_loss: 0.0756 - val_acc: 0.9939\n",
      "Epoch 191/300\n",
      "70784/74147 [===========================>..] - ETA: 0s - loss: 0.0767 - acc: 0.9939Epoch 00191: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0762 - acc: 0.9940 - val_loss: 0.0758 - val_acc: 0.9941\n",
      "Epoch 192/300\n",
      "68736/74147 [==========================>...] - ETA: 0s - loss: 0.0770 - acc: 0.9939Epoch 00192: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0762 - acc: 0.9940 - val_loss: 0.0755 - val_acc: 0.9943\n",
      "Epoch 193/300\n",
      "68992/74147 [==========================>...] - ETA: 0s - loss: 0.0769 - acc: 0.9939Epoch 00193: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0762 - acc: 0.9940 - val_loss: 0.0756 - val_acc: 0.9943\n",
      "Epoch 194/300\n",
      "70016/74147 [===========================>..] - ETA: 0s - loss: 0.0772 - acc: 0.9939Epoch 00194: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0763 - acc: 0.9940 - val_loss: 0.0759 - val_acc: 0.9943\n",
      "Epoch 195/300\n",
      "68480/74147 [==========================>...] - ETA: 0s - loss: 0.0773 - acc: 0.9939Epoch 00195: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0761 - acc: 0.9940 - val_loss: 0.0749 - val_acc: 0.9944\n",
      "Epoch 196/300\n",
      "73600/74147 [============================>.] - ETA: 0s - loss: 0.0779 - acc: 0.9939Epoch 00196: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0776 - acc: 0.9939 - val_loss: 0.0755 - val_acc: 0.9943\n",
      "Epoch 197/300\n",
      "73984/74147 [============================>.] - ETA: 0s - loss: 0.0768 - acc: 0.9939Epoch 00197: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0766 - acc: 0.9939 - val_loss: 0.0755 - val_acc: 0.9943\n",
      "Epoch 198/300\n",
      "74112/74147 [============================>.] - ETA: 0s - loss: 0.0764 - acc: 0.9939Epoch 00198: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0763 - acc: 0.9939 - val_loss: 0.0768 - val_acc: 0.9940\n",
      "Epoch 199/300\n",
      "70016/74147 [===========================>..] - ETA: 0s - loss: 0.0778 - acc: 0.9938Epoch 00199: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0768 - acc: 0.9939 - val_loss: 0.0764 - val_acc: 0.9941\n",
      "Epoch 200/300\n",
      "68736/74147 [==========================>...] - ETA: 0s - loss: 0.0770 - acc: 0.9939Epoch 00200: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0761 - acc: 0.9940 - val_loss: 0.0765 - val_acc: 0.9936\n",
      "Epoch 201/300\n",
      "70912/74147 [===========================>..] - ETA: 0s - loss: 0.0768 - acc: 0.9938Epoch 00201: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 12us/step - loss: 0.0764 - acc: 0.9939 - val_loss: 0.0753 - val_acc: 0.9942\n",
      "Epoch 202/300\n",
      "73344/74147 [============================>.] - ETA: 0s - loss: 0.0771 - acc: 0.9939Epoch 00202: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 11us/step - loss: 0.0768 - acc: 0.9939 - val_loss: 0.0756 - val_acc: 0.9942\n",
      "Epoch 203/300\n",
      "71424/74147 [===========================>..] - ETA: 0s - loss: 0.0763 - acc: 0.9940Epoch 00203: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0761 - acc: 0.9941 - val_loss: 0.0755 - val_acc: 0.9941\n",
      "Epoch 204/300\n",
      "71040/74147 [===========================>..] - ETA: 0s - loss: 0.0776 - acc: 0.9938Epoch 00204: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0774 - acc: 0.9938 - val_loss: 0.0775 - val_acc: 0.9940\n",
      "Epoch 205/300\n",
      "70784/74147 [===========================>..] - ETA: 0s - loss: 0.0767 - acc: 0.9939Epoch 00205: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0762 - acc: 0.9939 - val_loss: 0.0749 - val_acc: 0.9944\n",
      "Epoch 206/300\n",
      "69760/74147 [===========================>..] - ETA: 0s - loss: 0.0765 - acc: 0.9939Epoch 00206: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0758 - acc: 0.9940 - val_loss: 0.0753 - val_acc: 0.9944\n",
      "Epoch 207/300\n",
      "71552/74147 [===========================>..] - ETA: 0s - loss: 0.0759 - acc: 0.9939Epoch 00207: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0759 - acc: 0.9940 - val_loss: 0.0750 - val_acc: 0.9945\n",
      "Epoch 208/300\n",
      "70528/74147 [===========================>..] - ETA: 0s - loss: 0.0780 - acc: 0.9938Epoch 00208: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0775 - acc: 0.9939 - val_loss: 0.0772 - val_acc: 0.9942\n",
      "Epoch 209/300\n",
      "71040/74147 [===========================>..] - ETA: 0s - loss: 0.0765 - acc: 0.9938Epoch 00209: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0762 - acc: 0.9938 - val_loss: 0.0751 - val_acc: 0.9944\n",
      "Epoch 210/300\n",
      "72576/74147 [============================>.] - ETA: 0s - loss: 0.0765 - acc: 0.9940Epoch 00210: val_acc improved from 0.99450 to 0.99458, saving model to bestWeightsMLP1L50.hdf5\n",
      "74147/74147 [==============================] - 1s 11us/step - loss: 0.0762 - acc: 0.9941 - val_loss: 0.0749 - val_acc: 0.9946\n",
      "Epoch 211/300\n",
      "72448/74147 [============================>.] - ETA: 0s - loss: 0.0767 - acc: 0.9939Epoch 00211: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 11us/step - loss: 0.0763 - acc: 0.9940 - val_loss: 0.0758 - val_acc: 0.9941\n",
      "Epoch 212/300\n",
      "72448/74147 [============================>.] - ETA: 0s - loss: 0.0766 - acc: 0.9939Epoch 00212: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 11us/step - loss: 0.0762 - acc: 0.9939 - val_loss: 0.0748 - val_acc: 0.9945\n",
      "Epoch 213/300\n",
      "71296/74147 [===========================>..] - ETA: 0s - loss: 0.0767 - acc: 0.9939Epoch 00213: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 11us/step - loss: 0.0766 - acc: 0.9939 - val_loss: 0.0758 - val_acc: 0.9941\n",
      "Epoch 214/300\n",
      "69888/74147 [===========================>..] - ETA: 0s - loss: 0.0771 - acc: 0.9939Epoch 00214: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 11us/step - loss: 0.0763 - acc: 0.9940 - val_loss: 0.0750 - val_acc: 0.9944\n",
      "Epoch 215/300\n",
      "71424/74147 [===========================>..] - ETA: 0s - loss: 0.0761 - acc: 0.9939Epoch 00215: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 11us/step - loss: 0.0760 - acc: 0.9939 - val_loss: 0.0751 - val_acc: 0.9943\n",
      "Epoch 216/300\n",
      "72192/74147 [============================>.] - ETA: 0s - loss: 0.0790 - acc: 0.9937 - ETA: 0s - loss: 0.0797 - accEpoch 00216: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 11us/step - loss: 0.0788 - acc: 0.9937 - val_loss: 0.0756 - val_acc: 0.9940\n",
      "Epoch 217/300\n",
      "71168/74147 [===========================>..] - ETA: 0s - loss: 0.0777 - acc: 0.9937Epoch 00217: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 11us/step - loss: 0.0777 - acc: 0.9937 - val_loss: 0.0756 - val_acc: 0.9940\n",
      "Epoch 218/300\n",
      "71552/74147 [===========================>..] - ETA: 0s - loss: 0.0760 - acc: 0.9940Epoch 00218: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 11us/step - loss: 0.0760 - acc: 0.9941 - val_loss: 0.0748 - val_acc: 0.9945\n",
      "Epoch 219/300\n",
      "72448/74147 [============================>.] - ETA: 0s - loss: 0.0765 - acc: 0.9940Epoch 00219: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 11us/step - loss: 0.0762 - acc: 0.9940 - val_loss: 0.0757 - val_acc: 0.9941\n",
      "Epoch 220/300\n",
      "72064/74147 [============================>.] - ETA: 0s - loss: 0.0769 - acc: 0.9939Epoch 00220: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 11us/step - loss: 0.0766 - acc: 0.9939 - val_loss: 0.0750 - val_acc: 0.9944\n",
      "Epoch 221/300\n",
      "72064/74147 [============================>.] - ETA: 0s - loss: 0.0770 - acc: 0.9939Epoch 00221: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 11us/step - loss: 0.0767 - acc: 0.9939 - val_loss: 0.0767 - val_acc: 0.9940\n",
      "Epoch 222/300\n",
      "72064/74147 [============================>.] - ETA: 0s - loss: 0.0769 - acc: 0.9940- ETA: 0s - loss: 0.0814 - acc: 0.Epoch 00222: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 11us/step - loss: 0.0765 - acc: 0.9940 - val_loss: 0.0753 - val_acc: 0.9943\n",
      "Epoch 223/300\n",
      "70400/74147 [===========================>..] - ETA: 0s - loss: 0.0776 - acc: 0.9938Epoch 00223: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 11us/step - loss: 0.0768 - acc: 0.9939 - val_loss: 0.0748 - val_acc: 0.9944\n",
      "Epoch 224/300\n",
      "71424/74147 [===========================>..] - ETA: 0s - loss: 0.0765 - acc: 0.9940Epoch 00224: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 11us/step - loss: 0.0763 - acc: 0.9940 - val_loss: 0.0750 - val_acc: 0.9942\n",
      "Epoch 225/300\n",
      "71808/74147 [============================>.] - ETA: 0s - loss: 0.0776 - acc: 0.9939Epoch 00225: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 11us/step - loss: 0.0773 - acc: 0.9939 - val_loss: 0.0757 - val_acc: 0.9944\n",
      "Epoch 226/300\n",
      "72704/74147 [============================>.] - ETA: 0s - loss: 0.0766 - acc: 0.9940Epoch 00226: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 11us/step - loss: 0.0764 - acc: 0.9941 - val_loss: 0.0752 - val_acc: 0.9942\n",
      "Epoch 227/300\n",
      "72064/74147 [============================>.] - ETA: 0s - loss: 0.0780 - acc: 0.9939- ETA: 0s - loss: 0.0809 - acc: 0.9Epoch 00227: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 11us/step - loss: 0.0777 - acc: 0.9939 - val_loss: 0.0750 - val_acc: 0.9945\n",
      "Epoch 228/300\n",
      "72192/74147 [============================>.] - ETA: 0s - loss: 0.0790 - acc: 0.9938Epoch 00228: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 11us/step - loss: 0.0787 - acc: 0.9938 - val_loss: 0.0764 - val_acc: 0.9938\n",
      "Epoch 229/300\n",
      "72192/74147 [============================>.] - ETA: 0s - loss: 0.0766 - acc: 0.9939Epoch 00229: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 11us/step - loss: 0.0763 - acc: 0.9940 - val_loss: 0.0743 - val_acc: 0.9945\n",
      "Epoch 230/300\n",
      "72192/74147 [============================>.] - ETA: 0s - loss: 0.0765 - acc: 0.9940Epoch 00230: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 11us/step - loss: 0.0763 - acc: 0.9940 - val_loss: 0.0745 - val_acc: 0.9945\n",
      "Epoch 231/300\n",
      "71296/74147 [===========================>..] - ETA: 0s - loss: 0.0764 - acc: 0.9940Epoch 00231: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 11us/step - loss: 0.0764 - acc: 0.9941 - val_loss: 0.0747 - val_acc: 0.9945\n",
      "Epoch 232/300\n",
      "71040/74147 [===========================>..] - ETA: 0s - loss: 0.0777 - acc: 0.9938Epoch 00232: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 11us/step - loss: 0.0775 - acc: 0.9938 - val_loss: 0.0782 - val_acc: 0.9940\n",
      "Epoch 233/300\n",
      "71296/74147 [===========================>..] - ETA: 0s - loss: 0.0773 - acc: 0.9938Epoch 00233: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 11us/step - loss: 0.0772 - acc: 0.9938 - val_loss: 0.0764 - val_acc: 0.9939\n",
      "Epoch 234/300\n",
      "72832/74147 [============================>.] - ETA: 0s - loss: 0.0764 - acc: 0.9940Epoch 00234: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 11us/step - loss: 0.0764 - acc: 0.9940 - val_loss: 0.0755 - val_acc: 0.9943\n",
      "Epoch 235/300\n",
      "72704/74147 [============================>.] - ETA: 0s - loss: 0.0771 - acc: 0.9940Epoch 00235: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 11us/step - loss: 0.0769 - acc: 0.9940 - val_loss: 0.0759 - val_acc: 0.9946\n",
      "Epoch 236/300\n",
      "72704/74147 [============================>.] - ETA: 0s - loss: 0.0766 - acc: 0.9940Epoch 00236: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 11us/step - loss: 0.0764 - acc: 0.9940 - val_loss: 0.0760 - val_acc: 0.9944\n",
      "Epoch 237/300\n",
      "70528/74147 [===========================>..] - ETA: 0s - loss: 0.0771 - acc: 0.9938Epoch 00237: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 11us/step - loss: 0.0765 - acc: 0.9939 - val_loss: 0.0762 - val_acc: 0.9945\n",
      "Epoch 238/300\n",
      "68480/74147 [==========================>...] - ETA: 0s - loss: 0.0774 - acc: 0.9938Epoch 00238: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0763 - acc: 0.9940 - val_loss: 0.0758 - val_acc: 0.9946\n",
      "Epoch 239/300\n",
      "69632/74147 [===========================>..] - ETA: 0s - loss: 0.0769 - acc: 0.9939Epoch 00239: val_acc improved from 0.99458 to 0.99462, saving model to bestWeightsMLP1L50.hdf5\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0763 - acc: 0.9940 - val_loss: 0.0752 - val_acc: 0.9946\n",
      "Epoch 240/300\n",
      "69504/74147 [===========================>..] - ETA: 0s - loss: 0.0779 - acc: 0.9938Epoch 00240: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0773 - acc: 0.9939 - val_loss: 0.0767 - val_acc: 0.9943\n",
      "Epoch 241/300\n",
      "68480/74147 [==========================>...] - ETA: 0s - loss: 0.0780 - acc: 0.9940Epoch 00241: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0768 - acc: 0.9941 - val_loss: 0.0764 - val_acc: 0.9945\n",
      "Epoch 242/300\n",
      "73472/74147 [============================>.] - ETA: 0s - loss: 0.0770 - acc: 0.9940Epoch 00242: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0766 - acc: 0.9940 - val_loss: 0.0765 - val_acc: 0.9944\n",
      "Epoch 243/300\n",
      "70016/74147 [===========================>..] - ETA: 0s - loss: 0.0778 - acc: 0.9938Epoch 00243: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0768 - acc: 0.9939 - val_loss: 0.0764 - val_acc: 0.9944\n",
      "Epoch 244/300\n",
      "68992/74147 [==========================>...] - ETA: 0s - loss: 0.0772 - acc: 0.9939Epoch 00244: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0763 - acc: 0.9940 - val_loss: 0.0760 - val_acc: 0.9944\n",
      "Epoch 245/300\n",
      "69760/74147 [===========================>..] - ETA: 0s - loss: 0.0770 - acc: 0.9939Epoch 00245: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0763 - acc: 0.9940 - val_loss: 0.0759 - val_acc: 0.9944\n",
      "Epoch 246/300\n",
      "70016/74147 [===========================>..] - ETA: 0s - loss: 0.0776 - acc: 0.9939Epoch 00246: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0767 - acc: 0.9940 - val_loss: 0.0766 - val_acc: 0.9943\n",
      "Epoch 247/300\n",
      "69760/74147 [===========================>..] - ETA: 0s - loss: 0.0777 - acc: 0.9938Epoch 00247: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0771 - acc: 0.9939 - val_loss: 0.0767 - val_acc: 0.9945\n",
      "Epoch 248/300\n",
      "68864/74147 [==========================>...] - ETA: 0s - loss: 0.0773 - acc: 0.9939Epoch 00248: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0763 - acc: 0.9940 - val_loss: 0.0764 - val_acc: 0.9945\n",
      "Epoch 249/300\n",
      "69888/74147 [===========================>..] - ETA: 0s - loss: 0.0775 - acc: 0.9939Epoch 00249: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0765 - acc: 0.9941 - val_loss: 0.0764 - val_acc: 0.9945\n",
      "Epoch 250/300\n",
      "70656/74147 [===========================>..] - ETA: 0s - loss: 0.0768 - acc: 0.9940Epoch 00250: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0763 - acc: 0.9941 - val_loss: 0.0771 - val_acc: 0.9943\n",
      "Epoch 251/300\n",
      "69376/74147 [===========================>..] - ETA: 0s - loss: 0.0771 - acc: 0.9939Epoch 00251: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0766 - acc: 0.9940 - val_loss: 0.0772 - val_acc: 0.9943\n",
      "Epoch 252/300\n",
      "68736/74147 [==========================>...] - ETA: 0s - loss: 0.0775 - acc: 0.9938Epoch 00252: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0766 - acc: 0.9939 - val_loss: 0.0765 - val_acc: 0.9943\n",
      "Epoch 253/300\n",
      "69504/74147 [===========================>..] - ETA: 0s - loss: 0.0784 - acc: 0.9939- ETA: 0s - loss: 0.0811 - acc: 0.993Epoch 00253: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0777 - acc: 0.9940 - val_loss: 0.0765 - val_acc: 0.9943\n",
      "Epoch 254/300\n",
      "69760/74147 [===========================>..] - ETA: 0s - loss: 0.0771 - acc: 0.9939Epoch 00254: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0764 - acc: 0.9940 - val_loss: 0.0769 - val_acc: 0.9943\n",
      "Epoch 255/300\n",
      "69376/74147 [===========================>..] - ETA: 0s - loss: 0.0768 - acc: 0.9940Epoch 00255: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0762 - acc: 0.9941 - val_loss: 0.0764 - val_acc: 0.9946\n",
      "Epoch 256/300\n",
      "68864/74147 [==========================>...] - ETA: 0s - loss: 0.0773 - acc: 0.9939Epoch 00256: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0763 - acc: 0.9941 - val_loss: 0.0767 - val_acc: 0.9945\n",
      "Epoch 257/300\n",
      "68224/74147 [==========================>...] - ETA: 0s - loss: 0.0776 - acc: 0.9939Epoch 00257: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 9us/step - loss: 0.0768 - acc: 0.9940 - val_loss: 0.0764 - val_acc: 0.9944\n",
      "Epoch 258/300\n",
      "68736/74147 [==========================>...] - ETA: 0s - loss: 0.0776 - acc: 0.9939Epoch 00258: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 10us/step - loss: 0.0767 - acc: 0.9940 - val_loss: 0.0769 - val_acc: 0.9942\n",
      "Epoch 259/300\n",
      "69888/74147 [===========================>..] - ETA: 0s - loss: 0.0772 - acc: 0.9939Epoch 00259: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 15us/step - loss: 0.0763 - acc: 0.9940 - val_loss: 0.0766 - val_acc: 0.9944\n",
      "Epoch 260/300\n",
      "71552/74147 [===========================>..] - ETA: 0s - loss: 0.0765 - acc: 0.9939- ETA: 0s - loss: 0.0821 - acc: 0Epoch 00260: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 11us/step - loss: 0.0765 - acc: 0.9939 - val_loss: 0.0761 - val_acc: 0.9945\n",
      "Epoch 261/300\n",
      "72832/74147 [============================>.] - ETA: 0s - loss: 0.0763 - acc: 0.9940Epoch 00261: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 11us/step - loss: 0.0762 - acc: 0.9940 - val_loss: 0.0759 - val_acc: 0.9945\n",
      "Epoch 262/300\n",
      "72704/74147 [============================>.] - ETA: 0s - loss: 0.0767 - acc: 0.9939Epoch 00262: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 11us/step - loss: 0.0767 - acc: 0.9939 - val_loss: 0.0795 - val_acc: 0.9941\n",
      "Epoch 263/300\n",
      "72960/74147 [============================>.] - ETA: 0s - loss: 0.0769 - acc: 0.9939Epoch 00263: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 11us/step - loss: 0.0768 - acc: 0.9939 - val_loss: 0.0767 - val_acc: 0.9943\n",
      "Epoch 264/300\n",
      "71552/74147 [===========================>..] - ETA: 0s - loss: 0.0767 - acc: 0.9940Epoch 00264: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 11us/step - loss: 0.0767 - acc: 0.9940 - val_loss: 0.0775 - val_acc: 0.9941\n",
      "Epoch 265/300\n",
      "71424/74147 [===========================>..] - ETA: 0s - loss: 0.0788 - acc: 0.9938Epoch 00265: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 11us/step - loss: 0.0788 - acc: 0.9939 - val_loss: 0.0780 - val_acc: 0.9943\n",
      "Epoch 266/300\n",
      "71680/74147 [============================>.] - ETA: 0s - loss: 0.0784 - acc: 0.9939Epoch 00266: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 11us/step - loss: 0.0780 - acc: 0.9940 - val_loss: 0.0781 - val_acc: 0.9943\n",
      "Epoch 267/300\n",
      "73472/74147 [============================>.] - ETA: 0s - loss: 0.0777 - acc: 0.9938Epoch 00267: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 11us/step - loss: 0.0774 - acc: 0.9939 - val_loss: 0.0765 - val_acc: 0.9944\n",
      "Epoch 268/300\n",
      "70528/74147 [===========================>..] - ETA: 0s - loss: 0.0772 - acc: 0.9940Epoch 00268: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 11us/step - loss: 0.0767 - acc: 0.9941 - val_loss: 0.0767 - val_acc: 0.9943\n",
      "Epoch 269/300\n",
      "71936/74147 [============================>.] - ETA: 0s - loss: 0.0767 - acc: 0.9940Epoch 00269: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 11us/step - loss: 0.0764 - acc: 0.9940 - val_loss: 0.0761 - val_acc: 0.9945\n",
      "Epoch 270/300\n",
      "72448/74147 [============================>.] - ETA: 0s - loss: 0.0769 - acc: 0.9940- ETA: 0s - loss: 0.0781 - acc: 0.993Epoch 00270: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 11us/step - loss: 0.0765 - acc: 0.9941 - val_loss: 0.0766 - val_acc: 0.9945\n",
      "Epoch 271/300\n",
      "71936/74147 [============================>.] - ETA: 0s - loss: 0.0770 - acc: 0.9940Epoch 00271: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 11us/step - loss: 0.0767 - acc: 0.9940 - val_loss: 0.0768 - val_acc: 0.9944\n",
      "Epoch 272/300\n",
      "71296/74147 [===========================>..] - ETA: 0s - loss: 0.0763 - acc: 0.9941Epoch 00272: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 11us/step - loss: 0.0762 - acc: 0.9941 - val_loss: 0.0758 - val_acc: 0.9945\n",
      "Epoch 273/300\n",
      "71936/74147 [============================>.] - ETA: 0s - loss: 0.0779 - acc: 0.9938Epoch 00273: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 11us/step - loss: 0.0775 - acc: 0.9938 - val_loss: 0.0758 - val_acc: 0.9945\n",
      "Epoch 274/300\n",
      "70784/74147 [===========================>..] - ETA: 0s - loss: 0.0773 - acc: 0.9939Epoch 00274: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 11us/step - loss: 0.0772 - acc: 0.9939 - val_loss: 0.0777 - val_acc: 0.9936\n",
      "Epoch 275/300\n",
      "71552/74147 [===========================>..] - ETA: 0s - loss: 0.0776 - acc: 0.9935Epoch 00275: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 11us/step - loss: 0.0780 - acc: 0.9935 - val_loss: 0.0821 - val_acc: 0.9939\n",
      "Epoch 276/300\n",
      "72064/74147 [============================>.] - ETA: 0s - loss: 0.0774 - acc: 0.9940Epoch 00276: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 11us/step - loss: 0.0771 - acc: 0.9940 - val_loss: 0.0764 - val_acc: 0.9940\n",
      "Epoch 277/300\n",
      "71552/74147 [===========================>..] - ETA: 0s - loss: 0.0767 - acc: 0.9939Epoch 00277: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 11us/step - loss: 0.0767 - acc: 0.9939 - val_loss: 0.0748 - val_acc: 0.9943\n",
      "Epoch 278/300\n",
      "73856/74147 [============================>.] - ETA: 0s - loss: 0.0782 - acc: 0.9939Epoch 00278: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 11us/step - loss: 0.0779 - acc: 0.9939 - val_loss: 0.0750 - val_acc: 0.9943\n",
      "Epoch 279/300\n",
      "73472/74147 [============================>.] - ETA: 0s - loss: 0.0772 - acc: 0.9938Epoch 00279: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 11us/step - loss: 0.0767 - acc: 0.9939 - val_loss: 0.0746 - val_acc: 0.9945\n",
      "Epoch 280/300\n",
      "73600/74147 [============================>.] - ETA: 0s - loss: 0.0764 - acc: 0.9940Epoch 00280: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 11us/step - loss: 0.0761 - acc: 0.9940 - val_loss: 0.0747 - val_acc: 0.9945\n",
      "Epoch 281/300\n",
      "73728/74147 [============================>.] - ETA: 0s - loss: 0.0764 - acc: 0.9940Epoch 00281: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 11us/step - loss: 0.0760 - acc: 0.9940 - val_loss: 0.0749 - val_acc: 0.9945\n",
      "Epoch 282/300\n",
      "70400/74147 [===========================>..] - ETA: 0s - loss: 0.0766 - acc: 0.9939Epoch 00282: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 11us/step - loss: 0.0758 - acc: 0.9940 - val_loss: 0.0747 - val_acc: 0.9945\n",
      "Epoch 283/300\n",
      "70272/74147 [===========================>..] - ETA: 0s - loss: 0.0769 - acc: 0.9939Epoch 00283: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 11us/step - loss: 0.0760 - acc: 0.9940 - val_loss: 0.0756 - val_acc: 0.9943\n",
      "Epoch 284/300\n",
      "69120/74147 [==========================>...] - ETA: 0s - loss: 0.0768 - acc: 0.9939 - ETA: 0s - loss: 0.0774 - accEpoch 00284: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 11us/step - loss: 0.0761 - acc: 0.9940 - val_loss: 0.0750 - val_acc: 0.9944\n",
      "Epoch 285/300\n",
      "71168/74147 [===========================>..] - ETA: 0s - loss: 0.0776 - acc: 0.9937- ETA: 0s - loss: 0.0795 - acc: 0.9Epoch 00285: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 11us/step - loss: 0.0773 - acc: 0.9937 - val_loss: 0.0754 - val_acc: 0.9945\n",
      "Epoch 286/300\n",
      "71936/74147 [============================>.] - ETA: 0s - loss: 0.0761 - acc: 0.9940Epoch 00286: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 11us/step - loss: 0.0758 - acc: 0.9940 - val_loss: 0.0757 - val_acc: 0.9943\n",
      "Epoch 287/300\n",
      "71808/74147 [============================>.] - ETA: 0s - loss: 0.0763 - acc: 0.9940Epoch 00287: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 11us/step - loss: 0.0759 - acc: 0.9941 - val_loss: 0.0757 - val_acc: 0.9943\n",
      "Epoch 288/300\n",
      "72192/74147 [============================>.] - ETA: 0s - loss: 0.0767 - acc: 0.9939Epoch 00288: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 11us/step - loss: 0.0765 - acc: 0.9939 - val_loss: 0.0766 - val_acc: 0.9939\n",
      "Epoch 289/300\n",
      "70912/74147 [===========================>..] - ETA: 0s - loss: 0.0763 - acc: 0.9940Epoch 00289: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 11us/step - loss: 0.0759 - acc: 0.9941 - val_loss: 0.0745 - val_acc: 0.9943\n",
      "Epoch 290/300\n",
      "72192/74147 [============================>.] - ETA: 0s - loss: 0.0762 - acc: 0.9941Epoch 00290: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 11us/step - loss: 0.0759 - acc: 0.9941 - val_loss: 0.0750 - val_acc: 0.9943\n",
      "Epoch 291/300\n",
      "70784/74147 [===========================>..] - ETA: 0s - loss: 0.0767 - acc: 0.9940Epoch 00291: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 11us/step - loss: 0.0762 - acc: 0.9941 - val_loss: 0.0751 - val_acc: 0.9939\n",
      "Epoch 292/300\n",
      "71296/74147 [===========================>..] - ETA: 0s - loss: 0.0760 - acc: 0.9940Epoch 00292: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 12us/step - loss: 0.0760 - acc: 0.9940 - val_loss: 0.0747 - val_acc: 0.9945\n",
      "Epoch 293/300\n",
      "71168/74147 [===========================>..] - ETA: 0s - loss: 0.0759 - acc: 0.9941Epoch 00293: val_acc improved from 0.99462 to 0.99466, saving model to bestWeightsMLP1L50.hdf5\n",
      "74147/74147 [==============================] - 1s 11us/step - loss: 0.0757 - acc: 0.9941 - val_loss: 0.0744 - val_acc: 0.9947\n",
      "Epoch 294/300\n",
      "72448/74147 [============================>.] - ETA: 0s - loss: 0.0762 - acc: 0.9940Epoch 00294: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 11us/step - loss: 0.0758 - acc: 0.9941 - val_loss: 0.0751 - val_acc: 0.9943\n",
      "Epoch 295/300\n",
      "71552/74147 [===========================>..] - ETA: 0s - loss: 0.0766 - acc: 0.9939Epoch 00295: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 11us/step - loss: 0.0766 - acc: 0.9940 - val_loss: 0.0752 - val_acc: 0.9943\n",
      "Epoch 296/300\n",
      "70528/74147 [===========================>..] - ETA: 0s - loss: 0.0766 - acc: 0.9939- ETA: 0s - loss: 0.0783 - acc: 0.9Epoch 00296: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 11us/step - loss: 0.0760 - acc: 0.9940 - val_loss: 0.0753 - val_acc: 0.9943\n",
      "Epoch 297/300\n",
      "73088/74147 [============================>.] - ETA: 0s - loss: 0.0759 - acc: 0.9941Epoch 00297: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 11us/step - loss: 0.0759 - acc: 0.9941 - val_loss: 0.0752 - val_acc: 0.9943\n",
      "Epoch 298/300\n",
      "71040/74147 [===========================>..] - ETA: 0s - loss: 0.0764 - acc: 0.9940Epoch 00298: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 11us/step - loss: 0.0761 - acc: 0.9941 - val_loss: 0.0754 - val_acc: 0.9942\n",
      "Epoch 299/300\n",
      "72192/74147 [============================>.] - ETA: 0s - loss: 0.0762 - acc: 0.9940Epoch 00299: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 11us/step - loss: 0.0759 - acc: 0.9941 - val_loss: 0.0749 - val_acc: 0.9944\n",
      "Epoch 300/300\n",
      "71680/74147 [============================>.] - ETA: 0s - loss: 0.0764 - acc: 0.9940Epoch 00300: val_acc did not improve\n",
      "74147/74147 [==============================] - 1s 11us/step - loss: 0.0759 - acc: 0.9941 - val_loss: 0.0755 - val_acc: 0.9944\n",
      "--- 218.95636534690857 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#Implementing validation\n",
    "\n",
    "start_time = time.time()\n",
    "#x_pca = np.asarray(x_pca)\n",
    "#Y_train = onehotencode(Y_train)\n",
    "#Y_test = onehotencode(Y_test)\n",
    "x_train,y_train = processTrainTestArrays(X_train,Y_train)\n",
    "x_val,y_val = processTrainTestArrays(X_val,Y_val)\n",
    "model = create_model(x_train,y_train)\n",
    "model.fit(x_train, y_train, epochs=300, batch_size=128,shuffle=False,validation_data = (x_val,y_val),callbacks=[checkpoint])\n",
    "\n",
    "\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================\n",
      "Time for testing\n",
      "===========================\n",
      "\n",
      "Optimal weights loaded from file bestWeightsMLP1L50.hdf5\n",
      "Model Successfully compiled with loaded weights\n",
      "\n",
      "24716/24716 [==============================] - 1s 21us/step\n",
      "Loss for testing = 0.08244315392683861 and Accuracy for testing = 0.9938905971936567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\salman\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 score is 0.993209130129773\n",
      "Confusion matrix: \n",
      " Predicted      0     1    2    3\n",
      "Actual                          \n",
      "0          19400    15    2    0\n",
      "1              0  4854    6    4\n",
      "2              1    14  189    1\n",
      "3              1   102    0  122\n",
      "4              0     5    0    0\n",
      "n_classes is:  5\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEKCAYAAAAMzhLIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+fL/AVDDXHBG/FL0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x21aaaee6be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Test with best model\n",
    "#Load weights and compile again\n",
    "print(\"\\n===========================\\nTime for testing\\n===========================\\n\")\n",
    "model.load_weights(hdf5FileName)\n",
    "nadam = optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=nadam, metrics=['accuracy'])\n",
    "print(\"Optimal weights loaded from file {}\".format(hdf5FileName))\n",
    "print(\"Model Successfully compiled with loaded weights\\n\")\n",
    "\n",
    "#Do same preprocessing for test data\n",
    "x_test,y_test = processTrainTestArrays(X_test,Y_test)\n",
    "loss,acc = model.evaluate(x_test,y_test)\n",
    "print(\"Loss for testing = {} and Accuracy for testing = {}\".format(loss,acc))\n",
    "predicted = model.predict(x_test)\n",
    "compute_metrics(predicted, y_test)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#temp =[np.argmax(item)+1 for item in predicted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#temp1 = [np.argmax(item)+1 for item in Y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
